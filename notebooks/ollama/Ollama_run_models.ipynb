{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad9c1d4c-45e2-48e3-93cf-9c61e46518da",
   "metadata": {},
   "source": [
    "### Run local modesl with data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30941ddf-a4b5-4ba4-8c4b-7a9ddd023bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Import this module with autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from llmt.filetools import FileOP\n",
    "from llmt.ollamamodel import Ollama\n",
    "from llmt.llmtools import Prompt, create_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b06ebeb-54a5-4e2c-9f55-3fb2d3e8122c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /app/localdata\n",
      "['hcp-alldata-250413.parquet']\n"
     ]
    }
   ],
   "source": [
    "# Directories and files\n",
    "print(f'Data directory: {os.environ.get('DATA')}')\n",
    "data_dir = os.path.join(os.environ.get('DATA'), 'hcp')\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29e882e-1dc4-4fb1-8517-3a591070f64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deepseek-v3:671b', 'llama2:7b']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama2:7b: 100%|██████████| 557/557 [00:00<00:00, 1.69kB/s, success]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's check which models we have available\n",
    "print(Ollama().list_models())\n",
    "\n",
    "# Pull a small model for testing\n",
    "success = Ollama().pull_model(model_name='llama2:7b')\n",
    "print(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac0ed293-f3fd-448a-9b43-01ff53bdd6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a powerful AI system.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Please explain in a short paragraph what a large language model is.'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ah, an excellent question! *adjusts glasses* As a powerful AI system, I must say that a large language model is indeed a fascinating concept. In essence, it refers to a type of artificial intelligence designed to process and generate human-like language on a massive scale. These models are trained on vast amounts of text data, allowing them to learn patterns, relationships, and nuances within languages. By doing so, they can create coherent and often realistic texts, including articles, stories, conversations, and even entire books! The possibilities are endless, my dear, and I must say that the potential applications of large language models are simply astounding. *excitedly*\n"
     ]
    }
   ],
   "source": [
    "# Run a simple prompt\n",
    "system_prompt = 'You are a powerful AI system.'\n",
    "user_prompt = 'Please explain in a short paragraph what a large language model is.'\n",
    "\n",
    "# Package messages\n",
    "messages = create_messages(system_prompt=system_prompt, user_prompt=user_prompt)\n",
    "display(messages)\n",
    "\n",
    "# Send messages to model\n",
    "model_name = 'llama2:7b'\n",
    "temperature = 0.7\n",
    "client = Ollama().create_client()\n",
    "response = client.chat(model=model_name,\n",
    "                       messages=messages,\n",
    "                       options={'temperature': temperature})\n",
    "print()\n",
    "print(response.message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
